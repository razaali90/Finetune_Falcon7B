# Finetune_Falcon7B
We finetune Falcon7B on Guanaco Dataset

Things I learned from this 
- Parameter Efficient Fine-Tuning (PEFT) on Low-Resource Hardware: [Source Article](https://huggingface.co/blog/peft)
- Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA: [Source Article](https://huggingface.co/blog/4bit-transformers-bitsandbytes)
- Supervised fine-tuning (or SFT for short) is a crucial step in RLHF. TRL provides an easy-to-use API to create SFT models and train them with a few lines of code on custom datasets.

